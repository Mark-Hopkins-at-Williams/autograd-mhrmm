{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**\"Central to all neural networks in PyTorch is the autograd package.\"** -- The PyTorch website\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As much as we've presumably enjoyed taking derivatives so far, it turns out that there's an easier way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.tensor(3., requires_grad=True)\n",
    "y = x**2\n",
    "y.backward()   # computes partial derivatives of y with respect to ancestors\n",
    "print(x.grad)  # dy/dx = 2*x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What, torch could do derivatives this whole time? Yes. It can even do it over vectors and general tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([6., 8.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([3., 4.], requires_grad=True)\n",
    "y = (x**2).sum()\n",
    "y.backward()   # computes partial derivatives of y with respect to ancestors\n",
    "print(x.grad)  # dy/dx1 = 2*x1; dy/dx2 = 2*x2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: What will the following code produce?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([27., 48.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([3., 4.], requires_grad=True)\n",
    "y = (2*x**3).mean()\n",
    "y.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can also use the Chain Rule to compute more distant partial derivatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[18., 24.],\n",
      "        [30., 36.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.tensor([[1.,2.],[3.,4.]], requires_grad=True)\n",
    "y = x + 2\n",
    "z = y * y * 3\n",
    "out = z.sum()\n",
    "out.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To activate autograd (automatic gradient) for a particular tensor, you simply need to set ```requires_grad=True``` when constructing the tensor. This ensures that the tensor will keep track of any operations that involve itself or its descendants in a computation graph (i.e., causal diagram), such that it can compute any (reasonable) partial derivatives of its descendants with respect to itself. Recall the following problem, in which we examine how the area of this right triangle changes as we change $\\theta$, assuming that the base $b$ remains constant (and the triangle remains a right triangle).\n",
    "\n",
    "![right triangle](./img/triangle.png)\n",
    "\n",
    "Previously, we set up the following causal diagram to depict this situation:\n",
    "\n",
    "![causal diagram](./img/trianglegraph.png)\n",
    "\n",
    "In the causal model, the structural equations were:\n",
    "\n",
    "    c = b / cos(theta)\n",
    "    h = c * sin(theta)\n",
    "    A = (b * h) / 2\n",
    "\n",
    "And we determined that the partial derivative of ```A``` with respect to ```theta``` was:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def our_custom_made_gradient(b, theta):\n",
    "    return (b**2/2) / math.cos(theta)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do this with torch and autograd!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our custom solution: 16.0\n",
      "Autograd's solution: 16.0\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "theta = torch.tensor(math.pi/4., requires_grad=True)\n",
    "b = torch.tensor(4.)\n",
    "c = b / torch.cos(theta)\n",
    "h = c * torch.sin(theta)\n",
    "a = (h * b) / 2.\n",
    "a.backward()\n",
    "\n",
    "print('Our custom solution: {:.1f}'.format(our_custom_made_gradient(b, theta)))\n",
    "print(\"Autograd's solution: {:.1f}\".format(theta.grad))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool. So guess we never have to do math again.\n",
    "\n",
    "Notice that since we aren't interested in any partial derivatives with respect to ```b```, we don't bother to set ```requires_grad=True``` for that tensor. It wouldn't hurt, but it does use additional memory and makes things a bit slower.\n",
    "\n",
    "But what happens if we don't set ```requires_grad=True``` for ```theta```?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-48c8c01dd3e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "theta = torch.tensor(math.pi/4.)\n",
    "b = torch.tensor(4.)\n",
    "c = b / torch.cos(theta)\n",
    "h = c * torch.sin(theta)\n",
    "a = (h * b) / 2.\n",
    "a.backward()\n",
    "print(theta.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You get a bit of an ugly error, but now you know what it'll look like, just in case you get it in the future (which you probably will at some point). It's also worth exploring a bit how the ```requires_grad``` flag gets propagated to descendants and copies of a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta: True\n",
      "b: False\n",
      "c: True\n",
      "h: True\n",
      "a: True\n"
     ]
    }
   ],
   "source": [
    "theta = torch.tensor(math.pi/4., requires_grad=True)\n",
    "b = torch.tensor(4.)\n",
    "c = b / torch.cos(theta)\n",
    "h = c * torch.sin(theta)\n",
    "a = (h * b) / 2.\n",
    "print('theta: {}'.format(theta.requires_grad))\n",
    "print('b: {}'.format(b.requires_grad))\n",
    "print('c: {}'.format(c.requires_grad))\n",
    "print('h: {}'.format(h.requires_grad))\n",
    "print('a: {}'.format(a.requires_grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a tensor has an explicit ```requires_grad==True```, then all of its descendants in the causal diagram also do (but not siblings, etc.) Clones also acquire the property:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "theta = torch.tensor(math.pi/4., requires_grad=True)\n",
    "theta_clone = theta.clone()\n",
    "print(theta_clone.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But the actual behavior of clones may not be what you might expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dc/db = None\n",
      "dc/da = 8.0\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor(4., requires_grad=True)\n",
    "b = a.clone()\n",
    "c = b ** 2\n",
    "c.backward()\n",
    "print('dc/db = {}'.format(b.grad))\n",
    "print('dc/da = {}'.format(a.grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that PyTorch isn't really treating ```b``` as a real part of the computational graph, just as sort of a standin for ```a```. Often, when we're cloning a tensor, we'll really want to detach it from the original tensor's causal diagram, and then build a new causal diagram for the clone. To do this, we need the rather more verbose ```.clone().detach().requires_grad_(True)```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dc/db = 8.0\n",
      "dc/da = None\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor(4., requires_grad=True)\n",
    "b = a.clone().detach().requires_grad_(True)\n",
    "c = b ** 2\n",
    "c.backward()\n",
    "print('dc/db = {}'.format(b.grad))\n",
    "print('dc/da = {}'.format(a.grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Project 3, we implemented logistic regression. Let's try to use autograd instead of explicitly computing the gradient ourselves. First, we need some data. Let's just create some data using the \"underweight disease\" example from class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   height  mass  offset  response\n",
      "0     6.5   107       1         1\n",
      "1     5.4   251       1         0\n",
      "2     5.8   105       1         1\n",
      "3     4.1   167       1         0\n",
      "4     6.0   107       1         1\n",
      "5     6.4   181       1         0\n",
      "6     6.5   128       1         1\n",
      "7     5.7   149       1         0\n",
      "8     6.9   107       1         1\n",
      "9     5.6   132       1         0\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "import pandas as pd\n",
    "import torch\n",
    "def create_simple_data(how_many):\n",
    "    def create_datum():\n",
    "        height = randint(40,70) / 10.\n",
    "        mass = randint(100,300)\n",
    "        overweight = mass - 45*height\n",
    "        underweight = 40*height - mass - 120\n",
    "        if underweight > 0:\n",
    "            response = 1\n",
    "        else:\n",
    "            response = 0\n",
    "        datum = {'offset': 1, 'height': height, 'mass': mass,\n",
    "                 'response': response }\n",
    "        return datum\n",
    "    def create_datum_with_response(response):\n",
    "        datum = create_datum()\n",
    "        while(datum['response'] == response):\n",
    "            datum = create_datum()\n",
    "        return datum\n",
    "\n",
    "    data = []\n",
    "    for i in range(how_many // 2):\n",
    "        data.append(create_datum_with_response(0))\n",
    "        data.append(create_datum_with_response(1))\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "simple_train = create_simple_data(200)\n",
    "simple_test = create_simple_data(100)\n",
    "print(simple_train[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also create some functions for getting the evidence matrix and the response vector from this Pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATAFRAME:\n",
      "   height  mass  offset  response\n",
      "0     6.5   107       1         1\n",
      "1     5.4   251       1         0\n",
      "2     5.8   105       1         1\n",
      "3     4.1   167       1         0\n",
      "4     6.0   107       1         1\n",
      "\n",
      "EVIDENCE MATRIX:\n",
      "tensor([[  6.5000, 107.0000,   1.0000],\n",
      "        [  5.4000, 251.0000,   1.0000],\n",
      "        [  5.8000, 105.0000,   1.0000],\n",
      "        [  4.1000, 167.0000,   1.0000],\n",
      "        [  6.0000, 107.0000,   1.0000]], dtype=torch.float64)\n",
      "\n",
      "RESPONSE VECTOR:\n",
      "tensor([1, 0, 1, 0, 1])\n"
     ]
    }
   ],
   "source": [
    "def evidence_matrix(dataframe):\n",
    "    \"\"\"\n",
    "    Gets the evidence matrix (as a Torch tensor) from the \n",
    "    Pandas dataframe.\n",
    "   \n",
    "    \"\"\"\n",
    "    columns = list(dataframe.columns)\n",
    "    if 'Unnamed: 0' in columns:\n",
    "        columns.remove('Unnamed: 0')\n",
    "    columns.remove('response')\n",
    "    return torch.from_numpy(dataframe[columns].values)\n",
    "    \n",
    "def response_vector(dataframe):\n",
    "    \"\"\"\n",
    "    Gets the response vector (as a Torch tensor) from the \n",
    "    Pandas dataframe.\n",
    "    \n",
    "    \"\"\"\n",
    "    return torch.from_numpy(dataframe['response'].values)  \n",
    "\n",
    "print(\"DATAFRAME:\")\n",
    "print(simple_train[:5])\n",
    "print(\"\\nEVIDENCE MATRIX:\")\n",
    "print(evidence_matrix(simple_train[:5]))\n",
    "print(\"\\nRESPONSE VECTOR:\")\n",
    "print(response_vector(simple_train[:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll reuse some of the code from Project 3 for training a logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of trained model on train data: 0.990\n",
      "Accuracy of trained model on test data: 0.970\n"
     ]
    }
   ],
   "source": [
    "MAX_STEPS = 5000 # do not change\n",
    "PRECISION = 0.0000001  # do not change\n",
    "\n",
    "from descent import Environment, adagrad\n",
    "from logistic import LogisticRegressionModel\n",
    "\n",
    "def train_logistic_regression(data, task_factory):\n",
    "    \"\"\"\n",
    "    Trains a logistic regression model from a given Pandas DataFrame.\n",
    "    \n",
    "    The function returns a trained LogisticRegressionModel.\n",
    "    \n",
    "    \"\"\"  \n",
    "    X = evidence_matrix(data)\n",
    "    y = response_vector(data)\n",
    "    task = task_factory(X, y)\n",
    "    steps = adagrad(0.9, task)\n",
    "    result = steps[-1]\n",
    "    return LogisticRegressionModel(result.double())\n",
    "\n",
    "class LogisticRegressionTask(Environment):\n",
    "    \n",
    "    def __init__(self, X, y):\n",
    "        Environment.__init__(self, \n",
    "                             torch.ones(X.shape[1], dtype=torch.float64), \n",
    "                             PRECISION, MAX_STEPS)  # do not change\n",
    "        self.X = X.double()\n",
    "        self.y = y.double()\n",
    "    \n",
    "    def gradient(self, w):\n",
    "        Xt = torch.t(self.X)\n",
    "        Xw = torch.mv(self.X, w.double())\n",
    "        result = torch.mv(Xt, torch.sigmoid(Xw) - self.y)\n",
    "        return result\n",
    "\n",
    "lr = train_logistic_regression(simple_train, LogisticRegressionTask)\n",
    "print('Accuracy of trained model on train data: {:.3f}'.format(\n",
    "    lr.evaluate(evidence_matrix(simple_train), response_vector(simple_test))))\n",
    "print('Accuracy of trained model on test data: {:.3f}'.format(\n",
    "    lr.evaluate(evidence_matrix(simple_test), response_vector(simple_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, instead of figuring out the gradient ourselves and then expressing it in code (as done in the above .gradient method), could we just use autograd? Let's try:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Create an alternative version of ```LogisticRegressionTask``` (called ```LogisticRegressionTaskAutograd``` that computes its gradient using autograd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of trained model on train data: 0.990\n",
      "Accuracy of trained model on test data: 0.970\n"
     ]
    }
   ],
   "source": [
    "class LogisticRegressionTaskAutograd(Environment):\n",
    "    \n",
    "    def __init__(self, X, y):\n",
    "        w_size = X.shape[1]\n",
    "        Environment.__init__(self, \n",
    "                             torch.ones(w_size, requires_grad=True, \n",
    "                                        dtype=torch.float64), \n",
    "                             PRECISION, MAX_STEPS)  # do not change\n",
    "        self.X = X.double()\n",
    "        self.y = y.double()\n",
    "    \n",
    "    def gradient(self, w):\n",
    "        w = w.clone().detach().requires_grad_(True)\n",
    "        X = self.X.clone().detach()\n",
    "        y = self.y.clone().detach()\n",
    "        Xw = torch.mv(X, w)  \n",
    "        loss = torch.sum((1. - y) * Xw - \n",
    "                         torch.log(torch.sigmoid(Xw)))\n",
    "        loss.backward()\n",
    "        return w.grad\n",
    "    \n",
    "lr = train_logistic_regression(simple_train, LogisticRegressionTaskAutograd)\n",
    "print('Accuracy of trained model on train data: {:.3f}'.format(\n",
    "    lr.evaluate(evidence_matrix(simple_train), response_vector(simple_test))))\n",
    "print('Accuracy of trained model on test data: {:.3f}'.format(\n",
    "    lr.evaluate(evidence_matrix(simple_test), response_vector(simple_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For fun, we can check that the disease that affects **underweight and overweight** subjects cannot be modeled well by logistic regression. First, let's create that data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   height  mass  offset  response\n",
      "0     4.7   298       1         1\n",
      "1     5.5   242       1         0\n",
      "2     5.2   256       1         1\n",
      "3     4.0   117       1         0\n",
      "4     5.6   257       1         1\n",
      "5     4.2   149       1         0\n",
      "6     6.4   296       1         1\n",
      "7     6.0   239       1         0\n",
      "8     4.5   262       1         1\n",
      "9     6.8   238       1         0\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "import pandas as pd\n",
    "def create_xor_data(how_many):\n",
    "    def create_datum():\n",
    "        height = randint(40,70) / 10.\n",
    "        mass = randint(100,300)\n",
    "        income = randint(20,200)\n",
    "        overweight = mass - 45*height\n",
    "        underweight = 40*height - mass - 120\n",
    "        if underweight > 0 or overweight > 0:\n",
    "            response = 1\n",
    "        else:\n",
    "            response = 0\n",
    "        datum = {'offset': 1, 'height': height, 'mass': mass, \n",
    "                 'response': response }\n",
    "        return datum\n",
    "    data = []\n",
    "    for i in range(how_many // 2):\n",
    "        datum = create_datum()\n",
    "        while(datum['response'] == 0):\n",
    "            datum = create_datum()\n",
    "        data.append(datum)\n",
    "        datum = create_datum()\n",
    "        while(datum['response'] == 1):\n",
    "            datum = create_datum()\n",
    "        data.append(datum)       \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "xor_train = create_xor_data(1000)\n",
    "xor_test = create_xor_data(1000)\n",
    "print(xor_train[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, let's run logistic regression (with autograd!) on it, and watch it fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of trained model on train data: 0.670\n",
      "Accuracy of trained model on test data: 0.668\n"
     ]
    }
   ],
   "source": [
    "lr = train_logistic_regression(xor_train, LogisticRegressionTaskAutograd)\n",
    "print('Accuracy of trained model on train data: {:.3f}'.format(\n",
    "    lr.evaluate(evidence_matrix(xor_train), response_vector(xor_test))))\n",
    "print('Accuracy of trained model on test data: {:.3f}'.format(\n",
    "    lr.evaluate(evidence_matrix(xor_test), response_vector(xor_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Encode the example two-layer neural network from the lecture notes that can successfully model the \"underweight and overweight disease\" example, and check its gradients for correct and incorrect responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   1.0000,    0.0000,   -1.0000],\n",
      "        [   6.6000,    0.0000,   -6.6000],\n",
      "        [ 120.0000,    0.0000, -120.0000]], dtype=torch.float64)\n",
      "tensor([[24.0000],\n",
      "        [ 0.0000],\n",
      "        [ 1.0000]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "from torch import tensor\n",
    "x = tensor([1, 6.6, 120], dtype=torch.float64)\n",
    "w1 = tensor([[-120.,    0., 1],\n",
    "             [  40.,  -45., 0],\n",
    "             [  -1.,    1., 0]], dtype=torch.float64, requires_grad=True)\n",
    "pi1 = torch.mv(w1.t(), x)\n",
    "x1 = torch.clamp(pi1, min=0)\n",
    "w2 = tensor([[ 1.],\n",
    "             [ 1.],\n",
    "             [-1.]], dtype=torch.float64, requires_grad=True)\n",
    "pi2 = torch.mv(w2.t(), x1)\n",
    "\n",
    "#y = tensor(1.) \n",
    "y = tensor(0.)\n",
    "loss =  torch.sum((1. - y) * pi2 - torch.log(torch.sigmoid(pi2)))\n",
    "loss.backward()\n",
    "\n",
    "print(w1.grad)\n",
    "print(w2.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
